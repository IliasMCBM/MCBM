{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7392bd6c-6f64-46f7-8ff2-16b1b15fd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "with open(\"notebook_utils.py\", \"w\") as f:\n",
    "    f.write(r.text)\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/pip_helper.py\",\n",
    ")\n",
    "open(\"pip_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from pip_helper import pip_install\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "pip_install(\"--pre\", \"-U\", \"openvino>=2024.2.0\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\"--pre\", \"-U\", \"openvino-tokenizers[transformers]\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\",\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"gradio>=4.19\",\n",
    "    \"onnx<1.16.2\",\n",
    "    \"einops\",\n",
    "    \"transformers_stream_generator\",\n",
    "    \"tiktoken\",\n",
    "    \"transformers>=4.43.1\",\n",
    "    \"faiss-cpu\",\n",
    "    \"sentence_transformers\",\n",
    "    \"langchain>=0.2.0\",\n",
    "    \"langchain-community>=0.2.15\",\n",
    "    \"langchainhub\",\n",
    "    \"unstructured\",\n",
    "    \"scikit-learn\",\n",
    "    \"python-docx\",\n",
    "    \"pypdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1bd873-6ab0-4dca-b6d4-5d395226d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "# fetch model configuration\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "text_example_en_path = Path(\"text_example_en.pdf\")\n",
    "text_example_en = \"text_example_en.pdf\"\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "if not text_example_en_path.exists():\n",
    "    r = requests.get(url=text_example_en)\n",
    "    content = io.BytesIO(r.content)\n",
    "    with open(\"text_example_en.pdf\", \"wb\") as f:\n",
    "        f.write(content.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67deecf-429b-4683-8cbd-ec2026827eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from transformers import (\n",
    "    TextIteratorStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a60951b-baa9-4212-b909-990ee4251bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f3bd6d048e4c3e87007b0f83010891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Language:', disabled=True, options=('English',), value='English')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_config import (\n",
    "    SUPPORTED_EMBEDDING_MODELS,\n",
    "    SUPPORTED_RERANK_MODELS,\n",
    "    SUPPORTED_LLM_MODELS,\n",
    ")\n",
    "\n",
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Model Language:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "model_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63117b62-61d8-4173-8737-b87dd0677c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74211f8249fe48168c505311b3bee25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', disabled=True, index=19, options=('tiny-llama-1b-chat', 'llama-3.2-1b-instruct'…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_ids = [model_id for model_id, model_config in SUPPORTED_LLM_MODELS[model_language.value].items() if model_config.get(\"rag_prompt_template\")]\n",
    "\n",
    "llm_model_id = widgets.Dropdown(\n",
    "    options=llm_model_ids,\n",
    "    value=llm_model_ids[-1],\n",
    "    description=\"Model:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "llm_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5c0fbf-f644-47cc-be88-197a6ddc2255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected LLM model llama-3.1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "llm_model_configuration = SUPPORTED_LLM_MODELS[model_language.value][llm_model_id.value]\n",
    "print(f\"Selected LLM model {llm_model_id.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28893fef-1e05-43fa-81fd-38f1d9a0e6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e430cb081742da85d2d9cf11ab7908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c379805426dc4179820b1c512b669469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd970c4a25024edba03a614eae00d41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disable=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d402ffa8-763c-4a92-b0a2-ee9eb0fb9fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a0aa9e8cdb4967b7c149001bdb8962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable AWQ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enable_awq = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Enable AWQ\",\n",
    "    disabled=not prepare_int4_model.value,\n",
    ")\n",
    "display(enable_awq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f295373-746e-4781-8937-7278ccba6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model_id = llm_model_configuration[\"model_id\"]\n",
    "pt_model_name = llm_model_id.value.split(\"-\")[0]\n",
    "fp16_model_dir = Path(llm_model_id.value) / \"FP16\"\n",
    "int8_model_dir = Path(llm_model_id.value) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(llm_model_id.value) / \"INT4_compressed_weights\"\n",
    "\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format fp16\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(fp16_model_dir)\n",
    "    display(Markdown(\"*Export command:*\"))\n",
    "    display(Markdown(f\"{export_command}\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
    "    display(Markdown(\"*Export command:*\"))\n",
    "    display(Markdown(f\"{export_command}\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"zephyr-7b-beta\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"mistral-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"minicpm-2b-dpo\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"gemma-2b-it\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"notus-7b-v1\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"neural-chat-7b-v3-1\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"llama-2-chat-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"llama-3-8b-instruct\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"gemma-7b-it\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"chatglm2-6b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.72,\n",
    "        },\n",
    "        \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
    "        \"red-pajama-3b-chat\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.5,\n",
    "        },\n",
    "        \"qwen2.5-7b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-3b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-14b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-1.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"qwen2.5-0.5b-instruct\": {\"sym\": True, \"group_size\": 128, \"ratio\": 1.0},\n",
    "        \"default\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    model_compression_params = compression_configs.get(llm_model_id.value, compression_configs[\"default\"])\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = llm_model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
    "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    if enable_awq.value:\n",
    "        int4_compression_args += \" --awq --dataset wikitext2 --num-samples 128\"\n",
    "    export_command_base += int4_compression_args\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
    "    display(Markdown(\"*Export command:*\"))\n",
    "    display(Markdown(f\"{export_command}\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e9c251-4afc-4705-860c-a317eb176042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model with INT4 compressed weights is 5102.71 MB\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ffe0821-1160-4149-9731-e96f4401f082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ec5803a96d45e8b446be1fd67ebf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embedding Model:', disabled=True, options=('bge-m3', 'bge-small-en-v1.5', 'bge-large-en-…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_id = list(SUPPORTED_EMBEDDING_MODELS[model_language.value])\n",
    "\n",
    "embedding_model_id = widgets.Dropdown(\n",
    "    options=embedding_model_id,\n",
    "    value=embedding_model_id[0],\n",
    "    description=\"Embedding Model:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "embedding_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04097a99-82a3-4756-972d-00e507be28fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected bge-m3 model\n"
     ]
    }
   ],
   "source": [
    "embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[model_language.value][embedding_model_id.value]\n",
    "print(f\"Selected {embedding_model_id.value} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f36f90-6907-4be0-b27e-b5fefe05eda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ddacfb70f540619dbd3cd9941b5770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Rerank Model:', disabled=True, index=1, options=('bge-reranker-v2-m3', 'bge-reranker-lar…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_model_id = list(SUPPORTED_RERANK_MODELS)\n",
    "\n",
    "rerank_model_id = widgets.Dropdown(\n",
    "    options=rerank_model_id,\n",
    "    value=rerank_model_id[1],\n",
    "    description=\"Rerank Model:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "rerank_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d601833a-9660-4afd-8047-1a42060940be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected bge-reranker-large model\n"
     ]
    }
   ],
   "source": [
    "rerank_model_configuration = SUPPORTED_RERANK_MODELS[rerank_model_id.value]\n",
    "print(f\"Selected {rerank_model_id.value} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1997b3a7-cc43-4881-8058-a38db4f195aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task feature-extraction\".format(embedding_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(embedding_model_id.value)\n",
    "\n",
    "if not Path(embedding_model_id.value).exists():\n",
    "    ! $export_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b0257c3-c781-48c8-a9ea-39629c2bb5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e238631a0c354c7d97d93f219ab35355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Rerank Model:', disabled=True, index=1, options=('bge-reranker-v2-m3', 'bge-reranker-lar…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_model_id = list(SUPPORTED_RERANK_MODELS)\n",
    "\n",
    "rerank_model_id = widgets.Dropdown(\n",
    "    options=rerank_model_id,\n",
    "    value=rerank_model_id[1],\n",
    "    description=\"Rerank Model:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "rerank_model_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6eba601-91ca-4ab2-9e2c-333f4a674eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected bge-reranker-large model\n"
     ]
    }
   ],
   "source": [
    "rerank_model_configuration = SUPPORTED_RERANK_MODELS[rerank_model_id.value]\n",
    "print(f\"Selected {rerank_model_id.value} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f3191ea-de94-4ac4-9116-d37b2adbb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task text-classification\".format(rerank_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(rerank_model_id.value)\n",
    "\n",
    "if not Path(rerank_model_id.value).exists():\n",
    "    ! $export_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1797ff7d-cec9-4cb4-a23d-ec7de6dd67a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e70840ce0c4d8fa3b5f0e8c3a516e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=3, options=('CPU', 'GPU', 'NPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "embedding_device = device_widget()\n",
    "\n",
    "embedding_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bad45f7-a646-4c86-91e5-9db62d3cbf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model will be loaded to AUTO device for text embedding\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding model will be loaded to {embedding_device.value} device for text embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79355280-d2b7-463c-981f-85d414c6bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import optimize_bge_embedding\n",
    "\n",
    "USING_NPU = embedding_device.value == \"NPU\"\n",
    "\n",
    "\n",
    "npu_embedding_dir = embedding_model_id.value + \"-npu\"\n",
    "npu_embedding_path = Path(npu_embedding_dir) / \"openvino_model.xml\"\n",
    "if USING_NPU and not Path(npu_embedding_dir).exists():\n",
    "    shutil.copytree(embedding_model_id.value, npu_embedding_dir)\n",
    "    optimize_bge_embedding(Path(embedding_model_id.value) / \"openvino_model.xml\", npu_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "423a2c90-52b5-451b-8282-2b8e0b5bebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc6aadd186a4b7897a67170445e82ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=3, options=('CPU', 'GPU', 'NPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_device = device_widget()\n",
    "\n",
    "rerank_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21daa2c8-0305-48c1-a176-d00aedc3e335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerenk model will be loaded to AUTO device for text reranking\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rerenk model will be loaded to {rerank_device.value} device for text reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d68601-8e40-43b4-a44b-1d0e32f57190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dba28534f9e472a999278ae32fb352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "llm_device = device_widget(\"CPU\", exclude=[\"NPU\"])\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f6a7d8-96ae-432b-a2d2-ba6e84b164f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model will be loaded to CPU device for response generation\n"
     ]
    }
   ],
   "source": [
    "print(f\"LLM model will be loaded to {llm_device.value} device for response generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27a95e2a-df7e-4017-9ea1-8dc9f4adbf40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.008982331492006779, 0.03449303284287453, -0.024858957156538963]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenVINOBgeEmbeddings\n",
    "\n",
    "embedding_model_name = npu_embedding_dir if USING_NPU else embedding_model_id.value\n",
    "batch_size = 1 if USING_NPU else 4\n",
    "embedding_model_kwargs = {\"device\": embedding_device.value, \"compile\": False}\n",
    "encode_kwargs = {\n",
    "    \"mean_pooling\": embedding_model_configuration[\"mean_pooling\"],\n",
    "    \"normalize_embeddings\": embedding_model_configuration[\"normalize_embeddings\"],\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "\n",
    "embedding = OpenVINOBgeEmbeddings(\n",
    "    model_name_or_path=embedding_model_name,\n",
    "    model_kwargs=embedding_model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "if USING_NPU:\n",
    "    embedding.ov_model.reshape(1, 512)\n",
    "embedding.ov_model.compile()\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "embedding_result = embedding.embed_query(text)\n",
    "embedding_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dfe2785-de96-446a-afc0-def790ded159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name_or_path\" in OpenVINOReranker has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in OpenVINOReranker has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_compressors.openvino_rerank import OpenVINOReranker\n",
    "\n",
    "rerank_model_name = rerank_model_id.value\n",
    "rerank_model_kwargs = {\"device\": rerank_device.value}\n",
    "rerank_top_n = 2\n",
    "\n",
    "reranker = OpenVINOReranker(\n",
    "    model_name_or_path=rerank_model_name,\n",
    "    model_kwargs=rerank_model_kwargs,\n",
    "    top_n=rerank_top_n,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0305ddf-6c7d-42e9-b0ab-82cc7596164a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86d4e7cce8141b7b0fb5c6a1b79d7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4',), value='INT4')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "209a1acb-81ad-4659-b5dc-fda23a125f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from llama-3.1-8b-instruct\\INT4_compressed_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2 + 2 =?\\nThis'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {hints.performance_mode(): hints.PerformanceMode.LATENCY, streams.num(): \"1\", props.cache_dir(): \"\"}\n",
    "\n",
    "if \"GPU\" in llm_device.value and \"qwen2-7b-instruct\" in llm_model_id.value:\n",
    "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
    "\n",
    "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
    "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
    "if llm_model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and llm_device.value in [\"GPU\", \"AUTO\"]:\n",
    "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=str(model_dir),\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\n",
    "        \"device\": llm_device.value,\n",
    "        \"ov_config\": ov_config,\n",
    "        \"trust_remote_code\": True,\n",
    "    },\n",
    "    pipeline_kwargs={\"max_new_tokens\": 2},\n",
    ")\n",
    "\n",
    "if llm.pipeline.tokenizer.eos_token_id:\n",
    "    llm.pipeline.tokenizer.pad_token_id = llm.pipeline.tokenizer.eos_token_id\n",
    "\n",
    "llm.invoke(\"2 + 2 =\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09adf2ff-41f7-47da-b79a-65184f80f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    ")\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "    \"Markdown\": MarkdownTextSplitter,\n",
    "}\n",
    "\n",
    "\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "}\n",
    "\n",
    "if model_language.value == \"English\":\n",
    "    text_example_path = \"text_example_en.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "592da1ee-a9c3-4a2c-96db-76b12154ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vector database is Ready'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from threading import Thread\n",
    "import gradio as gr\n",
    "\n",
    "stop_tokens = llm_model_configuration.get(\"stop_tokens\")\n",
    "rag_prompt_template = llm_model_configuration[\"rag_prompt_template\"]\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = llm.pipeline.tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "\n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "\n",
    "def load_single_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    helper for loading a single document\n",
    "\n",
    "    Params:\n",
    "      file_path: document path\n",
    "    Returns:\n",
    "      documents loaded\n",
    "\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"File does not exist '{ext}'\")\n",
    "\n",
    "\n",
    "def default_partial_text_processor(partial_text: str, new_text: str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by default\n",
    "\n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "\n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "\n",
    "text_processor = llm_model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "\n",
    "def create_vectordb(\n",
    "    docs, spliter_name, chunk_size, chunk_overlap, vector_search_top_k, vector_rerank_top_n, run_rerank, search_method, score_threshold, progress=gr.Progress()\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize a vector database\n",
    "\n",
    "    Params:\n",
    "      doc: orignal documents provided by user\n",
    "      spliter_name: spliter method\n",
    "      chunk_size:  size of a single sentence chunk\n",
    "      chunk_overlap: overlap size between 2 chunks\n",
    "      vector_search_top_k: Vector search top k\n",
    "      vector_rerank_top_n: Search rerank top n\n",
    "      run_rerank: whether run reranker\n",
    "      search_method: top k search method\n",
    "      score_threshold: score threshold when selecting 'similarity_score_threshold' method\n",
    "\n",
    "    \"\"\"\n",
    "    global db\n",
    "    global retriever\n",
    "    global combine_docs_chain\n",
    "    global rag_chain\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        if type(doc) is not str:\n",
    "            doc = doc.name\n",
    "        documents.extend(load_single_document(doc))\n",
    "\n",
    "    text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = FAISS.from_documents(texts, embedding)\n",
    "    if search_method == \"similarity_score_threshold\":\n",
    "        search_kwargs = {\"k\": vector_search_top_k, \"score_threshold\": score_threshold}\n",
    "    else:\n",
    "        search_kwargs = {\"k\": vector_search_top_k}\n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=search_method)\n",
    "    if run_rerank:\n",
    "        reranker.top_n = vector_rerank_top_n\n",
    "        retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=retriever)\n",
    "    prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "    return \"Vector database is Ready\"\n",
    "\n",
    "\n",
    "def update_retriever(vector_search_top_k, vector_rerank_top_n, run_rerank, search_method, score_threshold):\n",
    "    \"\"\"\n",
    "    Update retriever\n",
    "\n",
    "    Params:\n",
    "      vector_search_top_k: Vector search top k\n",
    "      vector_rerank_top_n: Search rerank top n\n",
    "      run_rerank: whether run reranker\n",
    "      search_method: top k search method\n",
    "      score_threshold: score threshold when selecting 'similarity_score_threshold' method\n",
    "\n",
    "    \"\"\"\n",
    "    global db\n",
    "    global retriever\n",
    "    global combine_docs_chain\n",
    "    global rag_chain\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    if search_method == \"similarity_score_threshold\":\n",
    "        search_kwargs = {\"k\": vector_search_top_k, \"score_threshold\": score_threshold}\n",
    "    else:\n",
    "        search_kwargs = {\"k\": vector_search_top_k}\n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=search_method)\n",
    "    if run_rerank:\n",
    "        retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=retriever)\n",
    "        reranker.top_n = vector_rerank_top_n\n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "    return \"Vector database is Ready\"\n",
    "\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, hide_full_prompt, do_rag):\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      hide_full_prompt: whether to show searching results in promopt.\n",
    "      do_rag: whether do RAG when generating texts.\n",
    "\n",
    "    \"\"\"\n",
    "    streamer = TextIteratorStreamer(\n",
    "        llm.pipeline.tokenizer,\n",
    "        timeout=3600.0,\n",
    "        skip_prompt=hide_full_prompt,\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    pipeline_kwargs = dict(\n",
    "        max_new_tokens=512,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if stop_tokens is not None:\n",
    "        pipeline_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
    "\n",
    "    llm.pipeline_kwargs = pipeline_kwargs\n",
    "    if do_rag:\n",
    "        t1 = Thread(target=rag_chain.invoke, args=({\"input\": history[-1][0]},))\n",
    "    else:\n",
    "        input_text = rag_prompt_template.format(input=history[-1][0], context=\"\")\n",
    "        t1 = Thread(target=llm.invoke, args=(input_text,))\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = text_processor(partial_text, new_text)\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "\n",
    "def request_cancel():\n",
    "    llm.pipeline.model.request.cancel()\n",
    "\n",
    "\n",
    "# initialize the vector store with example document\n",
    "create_vectordb(\n",
    "    [text_example_path],\n",
    "    \"RecursiveCharacter\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    vector_search_top_k=10,\n",
    "    vector_rerank_top_n=2,\n",
    "    run_rerank=True,\n",
    "    search_method=\"similarity_score_threshold\",\n",
    "    score_threshold=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "947b5ba9-e7f2-45bf-952c-4e2285486103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(\n",
    "    load_doc_fn=create_vectordb,\n",
    "    run_fn=bot,\n",
    "    stop_fn=request_cancel,\n",
    "    update_retriever_fn=update_retriever,\n",
    "    model_name=llm_model_id.value,\n",
    "    language=model_language.value,\n",
    ")\n",
    "\n",
    "try:\n",
    "    demo.queue().launch()\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True)\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dcf2bde-bbe8-48b5-a4d3-783c8236bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34dfd73-2bf8-491d-a3aa-0253f1af36a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
